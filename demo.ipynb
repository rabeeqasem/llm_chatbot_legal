{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the api\n",
    "api_key=\"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\"\n",
    "import openai\n",
    "openai.api_key=api_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://github.com/jerryjliu/llama_index/blob/main/docs/examples/vector_stores/AsyncIndexCreationDemo.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import SimpleDirectoryReader,GPTListIndex,GPTVectorStoreIndex,LLMPredictor,PromptHelper,ServiceContext,StorageContext,load_index_from_storage\n",
    "from langchain import OpenAI\n",
    "import sys\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"OPENAI_API_KEY\"] = api_key\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_index(path):\n",
    "    max_input = 4096\n",
    "    tokens = 1000\n",
    "    chunk_size = 600 #for LLM, we need to define chunk size\n",
    "    max_chunk_overlap = 20\n",
    "    \n",
    "    #define prompt\n",
    "    promptHelper = PromptHelper(max_input,tokens,max_chunk_overlap,chunk_size_limit=chunk_size)\n",
    "    \n",
    "    #define LLM — there could be many models we can use, but in this example, let’s go with OpenAI model\n",
    "    #llmPredictor = LLMPredictor(llm=OpenAI(temperature=0, model_name=\"text-ada-001\",max_tokens=tokens))\n",
    "    llmPredictor = LLMPredictor(llm=OpenAI(temperature=0.7, model_name=\"text-davinci-003\",max_tokens=tokens))\n",
    "    \n",
    "    #load data — it will take all the .txtx files, if there are more than 1\n",
    "    docs = SimpleDirectoryReader(path).load_data()\n",
    "\n",
    "    #create vector index\n",
    "    service_context = ServiceContext.from_defaults(llm_predictor=llmPredictor,prompt_helper=promptHelper)\n",
    "        \n",
    "    vectorIndex = GPTVectorStoreIndex.from_documents(documents=docs,service_context=service_context)\n",
    "    vectorIndex.storage_context.persist(persist_dir = 'Store')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_index('text_files')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def answerMe(question):\n",
    "    storage_context = StorageContext.from_defaults(persist_dir = 'Store')\n",
    "    index = load_index_from_storage(storage_context)\n",
    "    query_engine = index.as_query_engine()\n",
    "    #response = query_engine.query(question)\n",
    "    response = query_engine.query(question) # increase the length of the prompt\n",
    "\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "هل من الضروري لجنة الادارة ان ترسل الى رئيس الهيئة نسخة عن وقائع الاجتماع الذي عقد؟\n",
      "\n",
      "لا، ليس من الضروري لجنة الإدارة أن ترسل نسخة من وقائع الاجتماع الذي عقد لرئيس الهيئة. ومع ذلك، يحق للجنة المراقبة حضور اجتماعات لجنة الإدارة في أي وقت تشاء (مادة 10). كما يحق للجنة المراقبة أن تطلب من الإدارة دعوة الهيئة العمومية للاجتماع إذا رأت أن هناك ما\n"
     ]
    }
   ],
   "source": [
    "promt=input(\"Ask me a question: \")\n",
    "answer=answerMe(promt)\n",
    "print(promt)\n",
    "print(answer.response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
